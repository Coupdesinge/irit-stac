#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Author: Eric Kow
# License: BSD3

"""
Print an interannotator agreement table:

* items:  segments (as defined by STAC) from *all* documents
* labels: their types (eg. Offer, Counteroffer, Refusal, Accept)
* metric: Cohen 1960 kappa as implemented in NLTK.

There is also an alternative --resource mode (see below)

Quick start
-----------

    cd Stac
    python code/agreement/unit-agreement data/pilot

Or perhaps more conveniently

    cd Stac/data
    python ../code/agreement/unit-agreement pilot

See the --help on

* --glob to restrict to a set of documents (eg. 'pilot2*')
* --verbose and --matrix for verbose and confusion matrix output

Resource mode
-------------
* items:  segments (as defined by STAC) from *all* documents
          whose type is 'Resource'
* labels: a tuple of resource type and status (eg. 'Clay:Givable')
* metric: (see above)

Details
-------
We compute the following set of scores for each document:

1. Pairwise agreement for each annotator that annotated the document

2. A "multi kappa" for the document itself, which uses "averages over observed
   and expected agreements for each coder pair." (Davis and Fleiss 1982).

   NLTK also offers the ability to compute plain kappa for multiple coders by
   taking an average over each coder pair.  We could switch to that or make it
   parameterisable as needed.

Notes:

* missing segments are given a default label (currently '?')
  (but the notion
* we use a crude 1/0 metric for agreement, but it might be nice to
  to do something which gives partial credit for the case when
  multiple labels are assigned, eg. (Strategic_comment/Other)
"""

from   glob      import glob
from   itertools import chain

import fnmatch
import itertools
import os
import sys

from nltk.metrics.agreement import AnnotationTask
from nltk.metrics           import ConfusionMatrix

from educe import stac
import showscores

def concat(xs):
    """
    Flatten a list of lists into a list.
    """
    return list(chain.from_iterable(xs))

def frozensingleton(x):
    return frozenset([x])

# ----------------------------------------------------------------------
# corpus slicing
# ----------------------------------------------------------------------

def concat_units(corpus, annotator):
    """
    Units from all documents for a given annotator excluding those which are
    structural annotations only (eg. paragraph, turn, etc)
    """
    all_units = [ v.units for k,v in corpus.items() if k.annotator == annotator ]
    return concat(all_units)

# ----------------------------------------------------------------------
# labeling task (dialogue act)
# ----------------------------------------------------------------------

def pretty_label(x):
    """
    Friendlier representation of label set
    (for use in confusion matrix output)
    """
    return "/".join(sorted(x))

def align_annotations(anno_units, mk_key=lambda x:x.position()):
    """
    Align annotations on the same set of documents from a set of annotators.
    Given a dictionary from annotator name to collection of units, return a
    a set of triples (annotator, key, item).

    Return the same number of triples for all annotators; if an item is
    missing for one annotator, we ignore it from all the other sets.
    """
    def mk_dict(xs):
        return { mk_key(x) : x for x in xs }

    def mk_triple(dict, annotator, key):
        unit = dicts[annotator].get(key, None)
        return (annotator, key, unit)

    dicts = { a : mk_dict(xs) for a,xs in anno_units.items() }
    annotators = dicts.keys()
    units      = dicts.values()
    keys       = frozenset.intersection(*[ frozenset(d.keys()) for d in units ])
    return concat([[mk_triple(dicts, a,k) for a in annotators] for k in keys])

# ----------------------------------------------------------------------
# options
# ----------------------------------------------------------------------

good_annotators = frozenset([ 'lpetersen', 'hjoseph' ])

import argparse

default_glob='pilot??'
arg_parser = argparse.ArgumentParser(description='Compute inter-annotator agreement.')
arg_parser.add_argument('corpus', metavar='DIR')
arg_parser.add_argument('--gold-ok',
                        default=False,
                        action='store_const',
                        const=True,
                        help='do not omit the GOLD annotator')
arg_parser.add_argument('--resource',
                        default=False,
                        action='store_const',
                        const=True,
                        help='compute agreement on resources instead')
arg_parser.add_argument('--glob',
                        default=default_glob,
                        help='documents to pick out (default: \'%s\')' % default_glob)
arg_parser.add_argument('--matrix', '-m',
                        default=False,
                        action='store_const',
                        const=True,
                        dest='matrix',
                        help='print confusion matrices')
arg_parser.add_argument('--verbose', '-v',
                        action='count',
                        default=1)
arg_parser.add_argument('--quiet' ,  '-q',
                        action='store_const',
                        const=0,
                        dest='verbose')
args=arg_parser.parse_args()

# ----------------------------------------------------------------------
# arg-sensitive helpers for corpus reading and agreement
# the functions here depend on the global args variable
# ----------------------------------------------------------------------

def select_items(corpus, annotators=None):
    if args.gold_ok:
        ok_annotators_ = good_annotators | frozensingleton('GOLD')
    else:
        ok_annotators_ = good_annotators

    if annotators is None:
        ok_annotators = ok_annotators_
    else:
        ok_annotators = ok_annotators_ & frozenset(annotators)

    def select_units(xs):
        if args.resource:
            return [ x for x in xs if stac.is_resource(x) ]
        else:
            return [ x for x in xs if stac.is_dialogue_act(x) ]

    return { a:select_units(concat_units(corpus, a)) for a in ok_annotators }

# we are building dictionaries from keys to annotation tasks
# a key is a tuple of documen set and some annotators (eg. a pair)
def mk_key(docs, annotators):
    return tuple(["/".join(sorted(docs))] + list(annotators))

def mk_triples(units):
    if args.resource:
        return mk_resource_triples(units)
    else:
        return mk_dialogue_act_triples(units)

# the set of triples behind an NLTK annotation task
# we also use these to build a confusion matrix for pairwise comparisons
def mk_dialogue_act_triples(units):
    if any(units.values()): # must have *some* annotations else div by 0
        aligned    = align_annotations(units)
        return [(c,i,stac.dialogue_act(u)) for c,i,u in aligned]
    else:
        return []

def mk_resource_triples(units_):
    def resources_only(xs):
        return [ x for x in xs if x.type == 'Resource' ]
    units = { k:resources_only(u) for k,u in units_.items() }
    if any(units.values()): # must have *some* annotations else div by 0
        aligned = align_annotations(units, mk_key=resource_key)
        label   = lambda u : frozensingleton("_".join(list(resource_label(u))))
        return [(c,i,label(u)) for c,i,u in aligned]
    else:
        return []

def mk_matrix(triples, a1, a2):
    if len(triples) > 0:
        l1 = [ pretty_label(l) for c,i,l in triples if c == a1 ]
        l2 = [ pretty_label(l) for c,i,l in triples if c == a2 ]
        return ConfusionMatrix(l1,l2)
    else:
        return None

def resource_key(x):
    return x.position() + ":" + x.features.get('Kind','?')

def resource_label(u):
    """
    Returns a tuple
    """
    #return (u.features["Kind"], u.features["Status"])
    return tuple([u.features["Status"]])

# ----------------------------------------------------------------------
# reading the corpus in and computing agreement
# ----------------------------------------------------------------------

def is_interesting(f):
   return fnmatch.fnmatch(f.doc, args.glob) and f.stage == 'units'

# dictionaries from FileId to file paths
reader       = stac.Reader(args.corpus)
corpus_files = reader.filter(reader.files(), is_interesting)

# actual contents of the files
# reading this in can be slow :-/
# is it the IO, do we need faster parsing?
# is it worth pickling the corpus?
corpus=reader.slurp(corpus_files, verbose=args.verbose)

if args.verbose:
    sys.stderr.write("Computing agreement\n")

docs = frozenset([k.doc for k in corpus.keys()])
big_tasks={}
tasks={}
confusion={}
for d in docs:
    # just the part of the corpus related to a specific document (all the
    # pilot21 annotations from all the annotators)
    dcorpus = reader.filter(corpus, lambda f:f.doc==d)
    all_items = select_items(dcorpus)
    # all here refers to all annotators that have touched this document
    all_annotators = all_items.keys()

    # pair-wise tasks
    for annotators in itertools.combinations(all_annotators, 2):
        some_items = select_items(dcorpus, annotators=annotators)
        key        = mk_key([d],annotators)
        triples    = mk_triples(some_items)
        matrix     = mk_matrix(triples, annotators[0], annotators[1])
        if matrix is not None:
            tasks[key]     = AnnotationTask(triples)
            confusion[key] = matrix

    # all-annotator tasks
    key            = mk_key([d],all_annotators)
    triples        = mk_triples(all_items)
    if len(triples) > 0:
        big_tasks[key] = AnnotationTask(triples)

# merged task: treat several documents as one set
merged_tasks     = {}
merged_confusion = {}
merged_corpus  = reader.filter(corpus, lambda f:f.annotator in good_annotators)
merged_docs    = frozenset([k.doc for k in merged_corpus.keys()])
for annotators in itertools.combinations(good_annotators, 2):
    some_items = select_items(merged_corpus, annotators=annotators)
    key        = mk_key(merged_docs,annotators)
    triples    = mk_triples(some_items)
    matrix     = mk_matrix(triples, annotators[0], annotators[1])
    if matrix is not None:
        merged_tasks[key]     = AnnotationTask(triples)
        merged_confusion[key] = matrix

# ----------------------------------------------------------------------
# displaying results
# ----------------------------------------------------------------------

def show_kappa_scores(ts, show):
    def score(key):
        try:
            # Davis and Fleiss 1982 - better (?) than kappa for > 2 annotators
            kappa = ts[key].multi_kappa()
        except:
            kappa = None
        return show(key, kappa)
    return "\n".join(map(score,sorted(ts.keys())))

def show_key(k):
    return " ".join(list(k))

if args.verbose > 1:
    for k in sorted(tasks.keys()):
        print showscores.banner(show_key(k))
        print tasks[k]
    print ""

    for k in sorted(big_tasks.keys()):
        print showscores.banner(show_key(k))
        print big_tasks[k]
    print ""

if args.matrix:
    print showscores.banner('confusion matrices')
    print ""
    for k in sorted(confusion.keys()):
        print show_key(k)
        print confusion[k]
        print ""

print showscores.banner('pair-wise scores')
print show_kappa_scores(tasks, showscores.show_pair)
print ""
print showscores.banner('all-annotator scores')
print show_kappa_scores(big_tasks, showscores.show_multi)

print showscores.banner('merged labeling scores')
print show_kappa_scores(merged_tasks, showscores.show_multi)
if args.matrix:
    print showscores.banner('merged confusion matrices')
    print ""
    for k in sorted(merged_confusion.keys()):
        print show_key(k)
        print merged_confusion[k]
        print ""

# vim: syntax=python
