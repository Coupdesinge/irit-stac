#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Author: Eric Kow
# License: BSD3

"""
Check the corpus for any consistency problems

Quick start
-----------

    cd Stac
    code/sanity-check data/pilot
"""

from collections import defaultdict

import educe.util
from educe import stac

import argparse
import os
import sys

arg_parser = argparse.ArgumentParser(description='Check corpus for potential problems.')
arg_parser.add_argument('corpus', metavar='DIR')
arg_parser.add_argument('un_corpus', metavar='DIR', help='Corpus w unannotated dirs [may be same]')
arg_parser.add_argument('--verbose', '-v',
                        action='count',
                        default=0)
educe.util.add_corpus_filters(arg_parser)
args=arg_parser.parse_args()
is_interesting=educe.util.mk_is_interesting(args)

def is_unannotated(k):
    return k.stage == 'unannotated' and is_interesting(k)

reader     = stac.Reader(args.corpus)
anno_files = reader.filter(reader.files(), is_interesting)
corpus     = reader.slurp(anno_files, verbose=True)

# bit of a mess: we have to read a separate batch of unannotated files
# because they were lost and may have to be reconstructed
un_reader     = stac.Reader(args.un_corpus)
un_anno_files = reader.filter(un_reader.files(), is_unannotated)
un_corpus     = reader.slurp(un_anno_files, verbose=True)


all_annos = {}
all_dupes = {}
for doc_id in corpus:
    annos_  = corpus[doc_id].units +\
              corpus[doc_id].relations +\
              corpus[doc_id].schemas
    annos = defaultdict(list)
    for x in annos_: annos[x.local_id()].append(x)

    dupes  = { k:v for k,v in annos.items() if len(v) > 1 }
    if len(dupes) > 0:
        all_dupes[doc_id] = dupes

if args.verbose and len(all_dupes) > 0:
    print "WARNING: duplicate annotation ids found"
    for k in sorted(all_dupes):
        descr = str(k)
        print descr
        print '-' * len(descr)
        print anno_files[k][0]
        print ""
        def show_anno(x):
            return "[%s] %s %s (%s)" % (x.type, x.span, x.features, corpus[k].text_for(x))
        for d in all_dupes[k]:
            id_str     = str(d)
            id_padding = ' ' * len(id_str)
            variants   = map(show_anno,all_dupes[k][d])
            print     " * %s: %s" % (id_str, variants[0])
            for v in variants[1:]:
                print "   %s  %s" % (id_padding, v)
        print ""
elif len(all_dupes) > 0:
    print "WARNING: duplicate annotation ids found in the following files:"
    for k in sorted(all_dupes, key=stac.id_to_path):
        print anno_files[k][0]


def has_match(unit, other_units):
    def is_me(unit2):
        # they won't have the same identifiers because the reconstructed
        # aa files are regenerated, and have different glozz identifiers
        return unit2.type == unit.type and\
               unit2.span == unit.span

    return len(filter(is_me, other_units)) > 0

for k in un_corpus:
    units = un_corpus[k].units
    for k2 in corpus:
        if k2.doc != k.doc or k2.subdoc != k.subdoc: continue
        units2 = corpus[k2].units
        for unit in units:
            txt = un_corpus[k].text_for(unit)
            if len(txt) > 50:
                txt = txt[1:50] + "..."
            if unit.type in stac.structure_types:
                if not has_match(unit, units2):
                    print "MISSING %s annotation %s in %s: [%s]" %\
                    (unit.type, unit.span, stac.id_to_path(k2), txt)
        for unit2 in units2:
            txt2 = corpus[k2].text_for(unit2)
            if len(txt) > 50:
                txt2 = txt2[1:50] + "..."
            if unit2.type in stac.structure_types:
                if not has_match(unit2, units):
                    print "PARASITE %s annotation %s in %s: [%s]" %\
                    (unit2.type, unit2.span, stac.id_to_path(k2), txt)
